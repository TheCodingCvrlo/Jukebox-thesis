{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/TheCodingCvrlo/bachelor-thesis/blob/main/best_model_training.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C80-5JG4CD9o",
        "outputId": "3247bd51-aad4-4508-a229-760afc27ff06"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"drive\", force_remount = True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NCzaWd47_cWI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c76d77e1-01e1-4e05-d8eb-5c6f60974021"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m24.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m184.3/184.3 kB\u001b[0m \u001b[31m20.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.7/211.7 kB\u001b[0m \u001b[31m23.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for pathtools (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "%pip install wandb --quiet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lFXIVAbtNmSb",
        "outputId": "74923415-41e2-4cfe-eeca-a9820863e3da"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing public.py.patched\n"
          ]
        }
      ],
      "source": [
        "#@title Patch to allow for continued wandb runs\n",
        "#patch extrapolated from the following open github issue https://github.com/wandb/wandb/issues/5194\n",
        "\n",
        "%%writefile public.py.patched\n",
        "diff --git a/public.py b/public.py\n",
        "index 4e961b6..8b68922 100644\n",
        "--- a/public.py\n",
        "+++ b/public.py\n",
        "@@ -2784,14 +2784,14 @@ class File(Attrs):\n",
        "         path = os.path.join(root, self.name)\n",
        "         if os.path.exists(path) and not replace:\n",
        "             if exist_ok:\n",
        "-                return open(path)\n",
        "+                return open(path, \"rb\")\n",
        "             else:\n",
        "                 raise ValueError(\n",
        "                     \"File already exists, pass replace=True to overwrite or exist_ok=True to leave it as is and don't error.\"\n",
        "                 )\n",
        "\n",
        "         util.download_file_from_url(path, self.url, Api().api_key)\n",
        "-        return open(path)\n",
        "+        return open(path, \"rb\")\n",
        "\n",
        "     @normalize_exceptions\n",
        "     def delete(self):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SHOE4pqZNovg",
        "outputId": "651c376d-e54f-4d1f-ef03-ce4182baa18e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "patching file /usr/local/lib/python3.10/dist-packages/wandb/apis/public.py\n"
          ]
        }
      ],
      "source": [
        "#apply patch\n",
        "!patch /usr/local/lib/python3.10/dist-packages/wandb/apis/public.py public.py.patched"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "2--HlnfeCXGd"
      },
      "outputs": [],
      "source": [
        "#@title Imports\n",
        "import os\n",
        "import sys\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import pickle\n",
        "from copy import deepcopy as cp\n",
        "from datetime import datetime\n",
        "\n",
        "#viz\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import wandb\n",
        "\n",
        "#pytorch\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.data import Dataset, Subset\n",
        "from torchvision.transforms import ToTensor, Lambda\n",
        "import torch.optim as optim\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HWKjBLSG2FB9"
      },
      "outputs": [],
      "source": [
        "\n",
        "PATH_CHECKS = \"drive//MyDrive//thesis//model_checkpoints\"\n",
        "PATHS_DATA = {\n",
        "    'pca30' : [\n",
        "        \"drive//MyDrive//thesis//data//reduced//30//train//features.csv\",\n",
        "        \"drive//MyDrive//thesis//data//reduced//30//train//labels.csv\",\n",
        "        \"drive//MyDrive//thesis//data//reduced//30//test//features.csv\",\n",
        "        \"drive//MyDrive//thesis//data//reduced//30//test//labels.csv\"\n",
        "        ],\n",
        "\n",
        "    'pca100' : [\n",
        "        \"drive//MyDrive//thesis//data//reduced//100//train//features.csv\",\n",
        "        \"drive//MyDrive//thesis//data//reduced//100//train//labels.csv\",\n",
        "        \"drive//MyDrive//thesis//data//reduced//100//test//features.csv\",\n",
        "        \"drive//MyDrive//thesis//data//reduced//100//test//labels.csv\"\n",
        "        ],\n",
        "    'full30' : \"drive//MyDrive//thesis//data//filtered//df_30.csv\",\n",
        "    'full100' : \"drive//MyDrive//thesis//data//filtered//df_100.csv\",\n",
        "}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NuIySWKPCojC",
        "outputId": "498b12d8-5428-4f21-f356-7709fe6ab78a",
        "cellView": "form"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using cuda device\n"
          ]
        }
      ],
      "source": [
        "#@title Device selection\n",
        "# default pytorch device selection snippet (credits https://pytorch.org/tutorials/beginner/basics/quickstart_tutorial.html)\n",
        "device = (\n",
        "    \"cuda\"\n",
        "    if torch.cuda.is_available()\n",
        "    else \"mps\"\n",
        "    if torch.backends.mps.is_available()\n",
        "    else \"cpu\"\n",
        ")\n",
        "\n",
        "print(f\"Using {device} device\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MMoD2AYq6rVw",
        "outputId": "e799c829-b402-4088-b81f-0b9ae8781246",
        "cellView": "form"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded dataset: pca30\n",
            "--------------------------\n",
            " N_FEATURES: 256 (reference: m)\n",
            " N_CLASSES: 137 (reference: k)\n",
            "--------------------------\n",
            " TRAIN SET SIZE: 9563\n",
            " TEST SET SIZE: 1688"
          ]
        }
      ],
      "source": [
        "#@title  Load df (test size only works for full reps)\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "dataset = \"pca30\" #@param [\"pca30\",\"pca100\",\"full30\",\"full100\"]{allow-input:true}\n",
        "test_size = 0.15 #@param {type:\"slider\", min:0, max:1, step:0.05}\n",
        "\n",
        "PATH_DF = PATHS_DATA[dataset]\n",
        "\n",
        "if isinstance(PATH_DF, str):\n",
        "\n",
        "\n",
        "  df = pd.read_csv(PATH_DF, index_col=0)\n",
        "\n",
        "  oh_enc = OneHotEncoder(sparse_output=False)\n",
        "  labels_oh = oh_enc.fit_transform(np.array(df['artist']).reshape(-1,1))\n",
        "\n",
        "  labels_df = pd.DataFrame(labels_oh, index=df.index, columns=['lab_'+i for i in oh_enc.categories_[0]])\n",
        "\n",
        "  df_full = pd.merge(df, labels_df, left_index=True, right_index=True)\n",
        "\n",
        "  feats_pattern = r'^\\d+$'\n",
        "  feats_cols = df.filter(regex=feats_pattern).columns\n",
        "  labs_cols = ['lab_'+i for i in oh_enc.categories_[0]]\n",
        "\n",
        "  targets = df_full.label\n",
        "\n",
        "  train_idx, valid_idx = train_test_split(\n",
        "      np.arange(len(targets)),\n",
        "      test_size=test_size,\n",
        "      shuffle=True,\n",
        "      stratify=targets\n",
        "      )\n",
        "\n",
        "  df_train = df_full.iloc[train_idx]\n",
        "  df_test = df_full.iloc[valid_idx]\n",
        "\n",
        "  train_feats = df_train[feats_cols].values\n",
        "  train_labs = df_train[labs_cols].values\n",
        "\n",
        "  test_feats = df_test[feats_cols].values\n",
        "  test_labs = df_test[labs_cols].values\n",
        "\n",
        "\n",
        "else:\n",
        "  feats_pattern = r'^\\d+$'\n",
        "\n",
        "  train_feats = pd.read_csv(PATH_DF[0], index_col=0).filter(regex=feats_pattern).values\n",
        "  test_feats = pd.read_csv(PATH_DF[2], index_col=0).filter(regex=feats_pattern).values\n",
        "\n",
        "  train_labs = pd.read_csv(PATH_DF[1], index_col=0).values\n",
        "  test_labs = pd.read_csv(PATH_DF[3], index_col=0).values\n",
        "\n",
        "n_train, m = train_feats.shape\n",
        "n_test, _ = test_feats.shape\n",
        "\n",
        "n = n_train + n_test\n",
        "\n",
        "_, k = train_labs.shape\n",
        "\n",
        "\n",
        "sys.stdout.write(f'Loaded dataset: {dataset}\\n--------------------------\\n')\n",
        "sys.stdout.write(f' N_FEATURES: {m} (reference: m)\\n N_CLASSES: {k} (reference: k)\\n--------------------------\\n')\n",
        "sys.stdout.write(f' TRAIN SET SIZE: {n_train}\\n TEST SET SIZE: {n_test}')\n",
        "\n",
        "# # df_torch = MyDataset(df_full,\n",
        "# #                      features_cols=feats_cols,\n",
        "# #                      target_cols=labs_cols,\n",
        "# #                      device=device)\n",
        "try:\n",
        "  del df, df_full, df_train, df_test\n",
        "except:\n",
        "  pass\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fYhdJD0DewUk"
      },
      "source": [
        "## Helper functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7XxtPapwQUC0"
      },
      "outputs": [],
      "source": [
        "#@title Dynamic Architecture Model\n",
        "class DNN(nn.Module):\n",
        "  \"\"\"\n",
        "  MLP with tunable number of layers and nodes. Declare sizes as a list with the following structure: [dim_in, dim_in_hl1, ..., dim_out].\n",
        "  Default dropout rate is 0.5, dropout happens after each activation (ReLU).\n",
        "  \"\"\"\n",
        "  def __init__(self, dropout=0.5, sizes=[m,k]):\n",
        "    super().__init__()\n",
        "    n = len(sizes)\n",
        "\n",
        "    stack = nn.ModuleList()\n",
        "    drop = nn.Dropout(dropout)\n",
        "    act = nn.ReLU()\n",
        "\n",
        "    self.dropout = drop\n",
        "    self.n_layers = n\n",
        "    self.activation = act\n",
        "\n",
        "    for i in range(n-1):\n",
        "      d_in = sizes[i]\n",
        "      d_out = sizes[i+1]\n",
        "\n",
        "      linear_layer = nn.Linear(d_in, d_out)\n",
        "\n",
        "      stack.append(linear_layer)\n",
        "      if i != n-2:\n",
        "        stack.append(act)\n",
        "        stack.append(drop)\n",
        "\n",
        "    self.stack = stack\n",
        "\n",
        "\n",
        "  def forward(self, x):\n",
        "    m = nn.Softmax(dim=0)\n",
        "\n",
        "    for layer in self.stack:\n",
        "      x = layer(x)\n",
        "\n",
        "    x = m(x)\n",
        "\n",
        "    return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "YA2j0PuOhX1_"
      },
      "outputs": [],
      "source": [
        "#@title dataset class\n",
        "\n",
        "class MyDataset(Dataset):\n",
        "\n",
        "  def __init__(self,x,y):\n",
        "    self.x=x\n",
        "    self.y=y\n",
        "    self.l = x.shape[0]\n",
        "\n",
        "  def __len__(self):\n",
        "    return self.l\n",
        "\n",
        "  def __getitem__(self,idx):\n",
        "    return self.x[idx], self.y[idx]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "oDOdU45h5gHj"
      },
      "outputs": [],
      "source": [
        "#@title get dataloaders\n",
        "def get_dataloaders(train_feats, test_feats, train_labels, test_labels, batch_size=1, shuffle=True, device=device):\n",
        "\n",
        "  train_dataset = MyDataset(torch.from_numpy(train_feats).float().to(device), torch.from_numpy(train_labels).float().to(device))\n",
        "  test_dataset = MyDataset(torch.from_numpy(test_feats).float().to(device), torch.from_numpy(test_labels).float().to(device))\n",
        "\n",
        "  train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=shuffle)\n",
        "  test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=shuffle)\n",
        "\n",
        "  return train_dataloader, test_dataloader\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zFN38pb152gb",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title run setup\n",
        "def run_setup(train_feats, test_feats, train_labels, test_labels, bs=64, shuffle=True, lr=1e-3, dropout=0.5, sizes=[m,k], device=device):\n",
        "  model = DNN(dropout=dropout, sizes=sizes).to(device)\n",
        "  optimizer = optim.SGD(model.parameters(), lr=lr)\n",
        "  # optimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "  train_dl, test_dl = get_dataloaders(train_feats,\n",
        "                                      test_feats,\n",
        "                                      train_labels,\n",
        "                                      test_labels,\n",
        "                                      batch_size=bs,\n",
        "                                      shuffle=True,\n",
        "                                      device=device)\n",
        "\n",
        "  return model, optimizer, train_dl, test_dl"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NkZz8u6UB6vm",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title train/test loop\n",
        "def train_loop(dataloader, model, loss_fn, optimizer):\n",
        "    size = len(dataloader.dataset)\n",
        "    num_batches = len(dataloader)\n",
        "    train_loss, train_acc = 0, 0\n",
        "\n",
        "    for batch, (X, y) in enumerate(dataloader):\n",
        "\n",
        "        # Compute prediction and loss\n",
        "        pred = model(X)\n",
        "        loss = loss_fn(pred, y)\n",
        "\n",
        "        # Backpropagation\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        train_loss += loss\n",
        "        train_acc += (pred.argmax(1) == y.argmax(1)).type(torch.float).sum().item()\n",
        "\n",
        "    train_loss /= num_batches\n",
        "    train_acc /= size\n",
        "\n",
        "    return train_loss, train_acc\n",
        "\n",
        "def test_loop(dataloader, model, loss_fn, verbose=False, t=1):\n",
        "    size = len(dataloader.dataset)\n",
        "    num_batches = len(dataloader)\n",
        "    test_loss, test_acc = 0, 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for X, y in dataloader:\n",
        "            pred = model(X)\n",
        "            test_loss += loss_fn(pred, y).item()\n",
        "            test_acc += (pred.argmax(1) == y.argmax(1)).type(torch.float).sum().item()\n",
        "\n",
        "\n",
        "    test_loss /= num_batches\n",
        "    test_acc /= size\n",
        "\n",
        "    if verbose and t%100 == 0:\n",
        "      sys.stdout.write('\\r' + f\"Validation accuracy: {(100*test_acc):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")\n",
        "\n",
        "    return test_loss, test_acc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fwx0t2ZjlVfS"
      },
      "outputs": [],
      "source": [
        "# #DEBUG\n",
        "# dlt, dls = get_dataloaders(train_feats, test_feats, train_labs, test_labs)\n",
        "# print('get_dataloaders ok')\n",
        "# dls_ls = [i for i in dls]\n",
        "# print(f'datatype: {dls_ls[0][0].dtype, dls_ls[0][1].dtype}')\n",
        "# mm, oo, dlt, dls = run_setup(train_feats, test_feats, train_labs, test_labs)\n",
        "# print(mm)\n",
        "# print(oo)\n",
        "# train_loop(dls, mm, nn.CrossEntropyLoss(), oo)\n",
        "# print('train_loop ok')\n",
        "# test_loop(dlt, mm,nn.CrossEntropyLoss())\n",
        "# print('checks ok!')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "psEqXQSCDEDv"
      },
      "source": [
        "# Training Best Model\n",
        "The following cells allow to train a model in different rounds, saving checkpoints and resuming the process while tracking each round within the same WandB run. Pretty swag."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "bs = 16\n",
        "lr = 6e-2\n",
        "dropout = 0.25\n",
        "sizes = [m, (m+k)//2, k]"
      ],
      "metadata": {
        "id": "QuftaRp5TbIF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model, optimizer, train_dataloader, test_dataloader = run_setup(train_feats,\n",
        "                                            test_feats,\n",
        "                                            train_labs,\n",
        "                                            test_labs,\n",
        "                                            bs = bs,\n",
        "                                            shuffle = True,\n",
        "                                            lr = lr,\n",
        "                                            dropout = dropout,\n",
        "                                            sizes = sizes,\n",
        "                                            device = device\n",
        "                                            )\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "kz_6ZDEwNO12"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = 200"
      ],
      "metadata": {
        "id": "e15W0VZ_TLi3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "os.listdir(f\"drive//MyDrive//thesis//model_checkpoints//jukebox-best-models//{dataset}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l7TrfRF3W27x",
        "outputId": "8ac0e006-fa06-419e-ab58-a9f3ef7bf872"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['dvt2xmy9_epoch_200']"
            ]
          },
          "metadata": {},
          "execution_count": 74
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GUPy4S4HB9EA",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 549
        },
        "outputId": "040b18ac-7560-498e-ef92-097305e638da"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Finishing last run (ID:sansy3uj) before initializing another..."
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
              "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
              "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>test/epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>test/test_acc</td><td>▁▂▂▃▂▄▃▃▂▄▄▅▄▃▅▂▄▂▃▃▆▄▆▄▃▅█▃▅▄▆▂▇▄▄▆▄▇█▃</td></tr><tr><td>test/test_loss</td><td>▇▅█▄▇▂▃█▇█▄▄▆▆▄▅▄▄▄▅▄▅▃▅▆▆▂▆▅▄▆▆▄▄▇▂▄▁▁▅</td></tr><tr><td>train/epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>train/train_accuracy</td><td>▁▁▂▂▂▃▂▃▃▄▄▄▄▃▄▄▅▅▅▅▆▅▅▅▆▆▆▆▆▆▇▆▇█▇▇▇███</td></tr><tr><td>train/train_loss</td><td>██▇▇▇▆▆▅▅▆▅▆▅▅▅▄▄▅▃▄▃▃▄▄▃▃▄▃▄▃▂▃▃▂▃▃▃▂▂▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>test/epoch</td><td>150</td></tr><tr><td>test/test_acc</td><td>0.50501</td></tr><tr><td>test/test_loss</td><td>3.02618</td></tr><tr><td>train/epoch</td><td>150</td></tr><tr><td>train/train_accuracy</td><td>0.75398</td></tr><tr><td>train/train_loss</td><td>2.86516</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">pca100_v3</strong> at: <a href='https://wandb.ai/torch-carlo/jukebox-best-models/runs/sansy3uj' target=\"_blank\">https://wandb.ai/torch-carlo/jukebox-best-models/runs/sansy3uj</a><br/>Synced 3 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20230708_204106-sansy3uj/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Successfully finished last run (ID:sansy3uj). Initializing new run:<br/>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.15.5"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20230708_205022-dvt2xmy9</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/torch-carlo/jukebox-best-models/runs/dvt2xmy9' target=\"_blank\">pca30_v1</a></strong> to <a href='https://wandb.ai/torch-carlo/jukebox-best-models' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/torch-carlo/jukebox-best-models' target=\"_blank\">https://wandb.ai/torch-carlo/jukebox-best-models</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/torch-carlo/jukebox-best-models/runs/dvt2xmy9' target=\"_blank\">https://wandb.ai/torch-carlo/jukebox-best-models/runs/dvt2xmy9</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training run dvt2xmy9\n",
            "--------------------------\n",
            "Epoch 101\n",
            "Validation accuracy: 37.1%, Avg loss: 4.338785 \n",
            "Epoch 201\n",
            "Validation accuracy: 38.3%, Avg loss: 4.352123 \n",
            "Done!\n"
          ]
        }
      ],
      "source": [
        "#@title Launch Training (4ca4dc012e66db4ce63df479d47564975c346ca3)\n",
        "from datetime import datetime\n",
        "\n",
        "PROJ_NAME = \"jukebox-best-models\"\n",
        "\n",
        "PATH_PROJ = PATH_CHECKS + f\"//{PROJ_NAME}\"\n",
        "os.makedirs(PATH_PROJ, exist_ok=True)\n",
        "run_name = \"pca30_v1\"\n",
        "\n",
        "run_id = None\n",
        "\n",
        "model = model.to(device)\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "epoch = 0\n",
        "\n",
        "dt = str(datetime.now().strftime('%b_%d_%Y-%H_%M'))\n",
        "\n",
        "if not run_id:\n",
        "   run_id = wandb.util.generate_id()\n",
        "\n",
        "\n",
        "chk = \"sansy3uj_epoch_50\"\n",
        "PATH_CHECK = PATH_PROJ+f\"//{dataset}\"\n",
        "os.makedirs(PATH_CHECK, exist_ok=True)\n",
        "PATH_CHECK += f\"//{chk}\"\n",
        "\n",
        "\n",
        "\n",
        "# comment \"resume\" parameter to initialize new run, comment \"reinit\" to load a checkpoint\n",
        "run = wandb.init(\n",
        "    project=PROJ_NAME,\n",
        "    name=run_name,\n",
        "    # resume='must',\n",
        "    id=run_id,\n",
        "    reinit=True\n",
        "    )\n",
        "\n",
        "\n",
        "if wandb.run.resumed:\n",
        "  checkpoint = torch.load(PATH_CHECK)\n",
        "  model.load_state_dict(checkpoint['model_state_dict'])\n",
        "  optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "  epoch = checkpoint['epoch']\n",
        "  train_loss = checkpoint['train_loss']\n",
        "  test_loss = checkpoint['test_loss']\n",
        "  train_acc = checkpoint['train_acc']\n",
        "  test_acc = checkpoint['test_acc']\n",
        "\n",
        "\n",
        "model.train()\n",
        "\n",
        "sys.stdout.write(f\"Training run {run_id}\\n--------------------------\\nEpoch {epoch}\")\n",
        "\n",
        "for t in range(epoch+1, epochs + epoch+1):\n",
        "    if t%100==0:\n",
        "      sys.stdout.write('\\r'+ f\"Epoch {t+1}\\n-------------------------------\")\n",
        "\n",
        "    train_loss, train_acc = train_loop(train_dataloader, model, loss_fn, optimizer)\n",
        "    test_loss, test_acc =  test_loop(test_dataloader, model, loss_fn, verbose=True, t=t)\n",
        "\n",
        "    metrics = {\n",
        "        'train/train_loss': train_loss,\n",
        "        'train/train_accuracy': train_acc,\n",
        "        'train/epoch' : t,\n",
        "        'test/epoch' : t,\n",
        "        'test/test_loss': test_loss,\n",
        "        'test/test_acc': test_acc\n",
        "    }\n",
        "\n",
        "    wandb.log(metrics)\n",
        "\n",
        "\n",
        "PATH_CHECK = PATH_PROJ+f'//{dataset}//{run_id}_epoch_{t}'\n",
        "\n",
        "torch.save({\n",
        "\n",
        "      'epoch': t,\n",
        "\n",
        "      'model_state_dict': model.state_dict(),\n",
        "      'optimizer_state_dict': optimizer.state_dict(),\n",
        "\n",
        "      'train_loss': train_loss,\n",
        "      'train_acc': train_acc,\n",
        "\n",
        "      'test_loss': test_loss,\n",
        "      'test_acc': test_acc,\n",
        "\n",
        "      }, PATH_CHECK)\n",
        "\n",
        "print(\"Done!\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "fYhdJD0DewUk"
      ],
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}